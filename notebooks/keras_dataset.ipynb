{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(filepaths, n_steps=100, batch_size=32):\n",
    "    \"\"\"Function that preprocesses the data.\"\"\"\n",
    "    \n",
    "    with open(filepaths) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    max_id = len(tokenizer.word_index)\n",
    "    print(max_id)\n",
    "    \n",
    "    [encoded] = np.array(tokenizer.texts_to_sequences([text])) - 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded)\n",
    "\n",
    "    window_length = n_steps + 1\n",
    "    dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "    dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "dataset = make_dataset(\"/home/tdelatte/Projects/Deep_Learning/NLP/Baudelaire_Poem_Generator/data/raw/baudelaire.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PrefetchDataset in module tensorflow.python.data.ops.dataset_ops object:\n",
      "\n",
      "class PrefetchDataset(UnaryUnchangedStructureDataset)\n",
      " |  PrefetchDataset(input_dataset, buffer_size)\n",
      " |  \n",
      " |  A `Dataset` that asynchronously prefetches its input.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PrefetchDataset\n",
      " |      UnaryUnchangedStructureDataset\n",
      " |      UnaryDataset\n",
      " |      DatasetV2\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dataset, buffer_size)\n",
      " |      See `Dataset.prefetch()` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DatasetV2:\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      The returned iterator implements the Python iterator protocol and therefore\n",
      " |      can only be used in eager mode.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If eager execution is not enabled.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Applies a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```\n",
      " |      dataset = (dataset.map(lambda x: x ** 2)\n",
      " |                 .apply(group_by_window(key_func, reduce_func, window_size))\n",
      " |                 .map(lambda x: x ** 3))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  batch(self, batch_size, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      The tensors in the resulting element will have an additional outer\n",
      " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      " |      element if `batch_size` does not divide the number of input elements `N`\n",
      " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
      " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
      " |      argument to `True` to prevent the smaller batch from being produced.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching tensors in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating given dataset with this dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6, 7 }\n",
      " |      \n",
      " |      # Input dataset and dataset to be concatenated should have same\n",
      " |      # nested structures and output types.\n",
      " |      # c = { (8, 9), (10, 11), (12, 13) }\n",
      " |      # d = { 14.0, 15.0, 16.0 }\n",
      " |      # a.concatenate(c) and a.concatenate(d) would result in error.\n",
      " |      \n",
      " |      a.concatenate(b) == { 1, 2, 3, 4, 5, 6, 7 }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      \n",
      " |      d = d.filter(lambda x: x < 3) # [1, 2]\n",
      " |      \n",
      " |      # `tf.math.equal(x, y)` is required for equality comparison\n",
      " |      def filter_fn(x):\n",
      " |        return tf.math.equal(x, 1)\n",
      " |      \n",
      " |      d = d.filter(filter_fn) # [1]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          scalar `tf.bool` tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` containing the elements of this dataset for which\n",
      " |            `predicate` is `True`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
      " |      stays the same. For example, to flatten a dataset of batches into a\n",
      " |      dataset of their elements:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset. '[...]' represents a tensor.\n",
      " |      a = {[1,2,3,4,5], [6,7,8,9], [10]}\n",
      " |      \n",
      " |      a.flat_map(lambda x: Dataset.from_tensor_slices(x)) ==\n",
      " |        {[1,2,3,4,5,6,7,8,9,10]}\n",
      " |      ```\n",
      " |      \n",
      " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
      " |      `flat_map` produces the same output as\n",
      " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length, block_length=1, num_parallel_calls=None)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Preprocess 4 files concurrently, and interleave blocks of 16 records from\n",
      " |      # each file.\n",
      " |      filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\", ...]\n",
      " |      dataset = (Dataset.from_tensor_slices(filenames)\n",
      " |                 .interleave(lambda x:\n",
      " |                     TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |                     cycle_length=4, block_length=16))\n",
      " |      ```\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      \n",
      " |      # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |                   cycle_length=2, block_length=4) == {\n",
      " |          1, 1, 1, 1,\n",
      " |          2, 2, 2, 2,\n",
      " |          1, 1,\n",
      " |          2, 2,\n",
      " |          3, 3, 3, 3,\n",
      " |          4, 4, 4, 4,\n",
      " |          3, 3,\n",
      " |          4, 4,\n",
      " |          5, 5, 5, 5,\n",
      " |          5, 5,\n",
      " |      }\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function. If\n",
      " |      `map_func` contains any stateful operations, the order in which\n",
      " |      that state is accessed is undefined.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |        cycle_length: The number of elements from this dataset that will be\n",
      " |          processed concurrently.\n",
      " |        block_length: The number of consecutive elements to produce from each\n",
      " |          input element before cycling to another input element.\n",
      " |        num_parallel_calls: (Optional.) If specified, the implementation creates\n",
      " |          a threadpool, which is used to fetch inputs from cycle elements\n",
      " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
      " |          from cycle elements synchronously with no parallelism. If the value\n",
      " |          `tf.data.experimental.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None)\n",
      " |      Maps `map_func` across the elements of this dataset.\n",
      " |      \n",
      " |      This transformation applies `map_func` to each element of this dataset, and\n",
      " |      returns a new dataset containing the transformed elements, in the same\n",
      " |      order as they appeared in the input.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      \n",
      " |      a.map(lambda x: x + 1) = { 2, 3, 4, 5, 6 }\n",
      " |      ```\n",
      " |      \n",
      " |      The input signature of `map_func` is determined by the structure of each\n",
      " |      element in this dataset. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Each element is a `tf.Tensor` object.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
      " |      # shape and dtype.\n",
      " |      result = a.map(lambda x: ...)\n",
      " |      \n",
      " |      # Each element is a tuple containing two `tf.Tensor` objects.\n",
      " |      b = { (1, \"foo\"), (2, \"bar\"), (3, \"baz\") }\n",
      " |      # `map_func` takes two arguments of type `tf.Tensor`.\n",
      " |      result = b.map(lambda x_int, y_str: ...)\n",
      " |      \n",
      " |      # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
      " |      c = { {\"a\": 1, \"b\": \"foo\"}, {\"a\": 2, \"b\": \"bar\"}, {\"a\": 3, \"b\": \"baz\"} }\n",
      " |      # `map_func` takes a single argument of type `dict` with the same keys as\n",
      " |      # the elements.\n",
      " |      result = c.map(lambda d: ...)\n",
      " |      ```\n",
      " |      \n",
      " |      The value or values returned by `map_func` determine the structure of each\n",
      " |      element in the returned dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      # `map_func` returns a scalar `tf.Tensor` of type `tf.float32`.\n",
      " |      def f(...):\n",
      " |        return tf.constant(37.0)\n",
      " |      result = dataset.map(f)\n",
      " |      result.output_classes == tf.Tensor\n",
      " |      result.output_types == tf.float32\n",
      " |      result.output_shapes == []  # scalar\n",
      " |      \n",
      " |      # `map_func` returns two `tf.Tensor` objects.\n",
      " |      def g(...):\n",
      " |        return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
      " |      result = dataset.map(g)\n",
      " |      result.output_classes == (tf.Tensor, tf.Tensor)\n",
      " |      result.output_types == (tf.float32, tf.string)\n",
      " |      result.output_shapes == ([], [3])\n",
      " |      \n",
      " |      # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
      " |      # `tf.Tensor`.\n",
      " |      def h(...):\n",
      " |        return 37.0, [\"Foo\", \"Bar\", \"Baz\"], np.array([1.0, 2.0] dtype=np.float64)\n",
      " |      result = dataset.map(h)\n",
      " |      result.output_classes == (tf.Tensor, tf.Tensor, tf.Tensor)\n",
      " |      result.output_types == (tf.float32, tf.string, tf.float64)\n",
      " |      result.output_shapes == ([], [3], [2])\n",
      " |      \n",
      " |      # `map_func` can return nested structures.\n",
      " |      def i(...):\n",
      " |        return {\"a\": 37.0, \"b\": [42, 16]}, \"foo\"\n",
      " |      result.output_classes == ({\"a\": tf.Tensor, \"b\": tf.Tensor}, tf.Tensor)\n",
      " |      result.output_types == ({\"a\": tf.float32, \"b\": tf.int32}, tf.string)\n",
      " |      result.output_shapes == ({\"a\": [], \"b\": [2]}, [])\n",
      " |      ```\n",
      " |      \n",
      " |      In addition to `tf.Tensor` objects, `map_func` can accept as arguments and\n",
      " |      return `tf.SparseTensor` objects.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having\n",
      " |          shapes and types defined by `self.output_shapes` and\n",
      " |         `self.output_types`) to another nested structure of tensors.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process asynchronously in parallel.\n",
      " |          If not specified, elements will be processed sequentially. If the value\n",
      " |          `tf.data.experimental.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  options(self)\n",
      " |      Returns the options for this dataset and its inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.data.Options` object representing the dataset options.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes, padding_values=None, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element.\n",
      " |      \n",
      " |      Like `tf.data.Dataset.batch`, the tensors in the resulting element will\n",
      " |      have an additional outer dimension, which will be `batch_size` (or\n",
      " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
      " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
      " |      your program depends on the batches having the same outer dimension, you\n",
      " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
      " |      batch from being produced.\n",
      " |      \n",
      " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
      " |      different shapes, and this transformation will pad each component to the\n",
      " |      respective shape in `padding_shapes`. The `padding_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant (e.g. `tf.Dimension(37)`), the component\n",
      " |        will be padded out to that length in that dimension.\n",
      " |      * If the dimension is unknown (e.g. `tf.Dimension(None)`), the component\n",
      " |        will be padded out to the maximum length of all elements in that\n",
      " |        dimension.\n",
      " |      \n",
      " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
      " |      elements that may have different shapes into a `tf.SparseTensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: A nested structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape\n",
      " |          to which the respective component of each input element should\n",
      " |          be padded prior to batching. Any unknown dimensions\n",
      " |          (e.g. `tf.Dimension(None)` in a `tf.TensorShape` or `-1` in a\n",
      " |          tensor-like object) will be padded to the maximum size of that\n",
      " |          dimension in each batch.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the\n",
      " |          respective components.  Defaults are `0` for numeric types and\n",
      " |          the empty string for string types.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          maximum number of elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  reduce(self, initial_state, reduce_func)\n",
      " |      Reduces the input dataset to a single element.\n",
      " |      \n",
      " |      The transformation calls `reduce_func` successively on every element of\n",
      " |      the input dataset until the dataset is exhausted, aggregating information in\n",
      " |      its internal state. The `initial_state` argument is used for the initial\n",
      " |      state and the final state is returned as the result.\n",
      " |      \n",
      " |      For example:\n",
      " |      - `tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1)`\n",
      " |        produces `5`\n",
      " |      - `tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y)`\n",
      " |        produces `10`\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_state: A nested structure of tensors, representing the initial\n",
      " |          state of the transformation.\n",
      " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
      " |          `new_state`. It must take two arguments and return a nested structure\n",
      " |          of tensors. The structure of `new_state` must match the structure of\n",
      " |          `initial_state`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.Tensor` objects, corresponding to the final\n",
      " |        state of the transformation.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset `count` times.\n",
      " |      \n",
      " |      NOTE: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior\n",
      " |          (if `count` is `None` or `-1`) is for the dataset be repeated\n",
      " |          indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can skip elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(input_file)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(pattern)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
      " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
      " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
      " |          placeholder tensor bypasses the early checking, and will instead result\n",
      " |          in an error during a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      " |      samples elements from this buffer, replacing the selected elements with new\n",
      " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      " |      full size of the dataset is required.\n",
      " |      \n",
      " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      " |      set to 1,000, then `shuffle` will initially select a random element from\n",
      " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
      " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      " |      maintaining the 1,000 element buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of elements from this dataset from which the new\n",
      " |          dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          random seed that will be used to create the distribution. See\n",
      " |          `tf.set_random_seed` for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number\n",
      " |          of elements of this dataset that should be skipped to form the\n",
      " |          new dataset.  If `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain no elements.  If `count`\n",
      " |          is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  window(self, size, shift=None, stride=1, drop_remainder=False)\n",
      " |      Combines input elements into a dataset of windows.\n",
      " |      \n",
      " |      Each window is a dataset itself and contains `size` elements (or\n",
      " |      possibly fewer if there are not enough input elements to fill the window\n",
      " |      and `drop_remainder` evaluates to false).\n",
      " |      \n",
      " |      The `stride` argument determines the stride of the input elements,\n",
      " |      and the `shift` argument determines the shift of the window.\n",
      " |      \n",
      " |      For example:\n",
      " |      - `tf.data.Dataset.range(7).window(2)` produces\n",
      " |        `{{0, 1}, {2, 3}, {4, 5}, {6}}`\n",
      " |      - `tf.data.Dataset.range(7).window(3, 2, 1, True)` produces\n",
      " |        `{{0, 1, 2}, {2, 3, 4}, {4, 5, 6}}`\n",
      " |      - `tf.data.Dataset.range(7).window(3, 1, 2, True)` produces\n",
      " |        `{{0, 2, 4}, {1, 3, 5}, {2, 4, 6}}`\n",
      " |      \n",
      " |      Args:\n",
      " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
      " |          of the input dataset to combine into a window.\n",
      " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          forward shift of the sliding window in each iteration. Defaults to\n",
      " |          `size`.\n",
      " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          stride of the input elements in the sliding window.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether a window should be dropped in case its size is smaller than\n",
      " |          `window_size`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of windows, each of which is a nested `Dataset` with\n",
      " |          the same structure as this dataset, but a finite subsequence of its\n",
      " |          elements.\n",
      " |  \n",
      " |  with_options(self, options)\n",
      " |      Returns a new `tf.data.Dataset` with the given options set.\n",
      " |      \n",
      " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
      " |      If options are set multiple times, they are merged as long as different\n",
      " |      options do not use different non-default values.\n",
      " |      \n",
      " |      Args:\n",
      " |        options: A `tf.data.Options` that identifies the options the use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` with the given options.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when an option is set more than once to a non-default value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from DatasetV2:\n",
      " |  \n",
      " |  from_generator(generator, output_types, output_shapes=None, args=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`.\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that support the `iter()` protocol (e.g. a generator function).\n",
      " |      The elements generated by `generator` must be compatible with the given\n",
      " |      `output_types` and (optional) `output_shapes` arguments.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      import itertools\n",
      " |      tf.enable_eager_execution()\n",
      " |      \n",
      " |      def gen():\n",
      " |        for i in itertools.count(1):\n",
      " |          yield (i, [1] * i)\n",
      " |      \n",
      " |      ds = tf.data.Dataset.from_generator(\n",
      " |          gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\n",
      " |      \n",
      " |      for value in ds.take(2):\n",
      " |        print value\n",
      " |      # (1, array([1]))\n",
      " |      # (2, array([1, 1]))\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The current implementation of `Dataset.from_generator()` uses\n",
      " |      `tf.py_func` and inherits the same constraints. In particular, it\n",
      " |      requires the `Dataset`- and `Iterator`-related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      NOTE: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that returns an object that supports the\n",
      " |          `iter()` protocol. If `args` is not specified, `generator` must take\n",
      " |          no arguments; otherwise it must take as many arguments as there are\n",
      " |          values in `args`.\n",
      " |        output_types: A nested structure of `tf.DType` objects corresponding to\n",
      " |          each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape`\n",
      " |          objects corresponding to each component of an element yielded by\n",
      " |          `generator`.\n",
      " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      " |          and passed to `generator` as NumPy-array arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this guide](\n",
      " |      https://tensorflow.org/guide/datasets#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors, each having the same size in the\n",
      " |          0th dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this\n",
      " |      guide](https://tensorflow.org/guide/datasets#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None, seed=None)\n",
      " |      A dataset of all files matching one or more glob patterns.\n",
      " |      \n",
      " |      NOTE: The default behavior of this method is to return filenames in\n",
      " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
      " |      to get results in a deterministic order.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset would\n",
      " |        produce:\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
      " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
      " |          pattern(s) that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.set_random_seed` for behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      Dataset.range(5) == [0, 1, 2, 3, 4]\n",
      " |      Dataset.range(2, 5) == [2, 3, 4]\n",
      " |      Dataset.range(1, 5, 2) == [1, 3]\n",
      " |      Dataset.range(1, 5, -2) == []\n",
      " |      Dataset.range(5, 1) == []\n",
      " |      Dataset.range(5, 1, -2) == [5, 3]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follows the same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1, stop = args[2]\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6 }\n",
      " |      c = { (7, 8), (9, 10), (11, 12) }\n",
      " |      d = { 13, 14 }\n",
      " |      \n",
      " |      # The nested structure of the `datasets` argument determines the\n",
      " |      # structure of elements in the resulting dataset.\n",
      " |      Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }\n",
      " |      Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }\n",
      " |      \n",
      " |      # The `datasets` argument may contain an arbitrary number of\n",
      " |      # datasets.\n",
      " |      Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),\n",
      " |                                  (2, 5, (9, 10)),\n",
      " |                                  (3, 6, (11, 12)) }\n",
      " |      \n",
      " |      # The number of elements in the resulting dataset is the same as\n",
      " |      # the size of the smallest dataset in `datasets`.\n",
      " |      Dataset.zip((a, d)) == { (1, 13), (2, 14) }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetV2:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
